{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you're instering the cosine distances you've previously calculated. If you only have the cosine similarity, you can compute it like this: \n",
    "```python \n",
    "1 - cosine similarity\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distances are the calculated cosine distances of the data points\n",
    "distances = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1: Mean-N Rarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method takes a list of distances, sorts and selects the top n_neighbours distances from each inner list, calculates the average for each set of selected distances, and then normalizes these average values between 0 and 1. The resulting rarity_core list contains the normalized values that represent the rarity of the data in each inner list of distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here you can set the threshold value n_neighbours\n",
    "n_neighbours = 100\n",
    "\n",
    "# sort the scores and take the first n_neighbours values\n",
    "sorted_distances = [sorted(distance_array)[:n_neighbours] for distance_array in distances]\n",
    "\n",
    "# calculate the sum of values and the number of instances in each array\n",
    "value_sums = [sum(distance_array) for distance_array in sorted_distances]\n",
    "instance_counts = [len(distance_array) for distance_array in sorted_distances]\n",
    "\n",
    "# calculate the average for each array\n",
    "averages = [value_sum / instance_count for value_sum, instance_count in zip(value_sums, instance_counts)]\n",
    "\n",
    "# normalization and transformation\n",
    "rarity_score = averages\n",
    "min_score = min(rarity_score)\n",
    "max_score = max(rarity_score)\n",
    "rarity_score = (rarity_score - min_score) / (max_score - min_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: Flow Rarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method computes inward flows for each data point of the distance array based on distance with a decay parameter, reverses the flow values, normalizes them between 0 and 1, and stores the resulting values in the rarity_score_flow variable. As a result you get the rarity score of each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute flows based on distance with a decay parameter\n",
    "def compute_flows(distance, decay=10):\n",
    "    return np.exp(-decay * distance)\n",
    "\n",
    "# sort the scores to get their sorted indices\n",
    "sorted_ids = np.argsort(distances)\n",
    "\n",
    "# here you can set the threshold value n_next_hubs\n",
    "n_next_hubs = 100\n",
    "\n",
    "# iterative flow search\n",
    "inward_flow_results = np.zeros(len(distances))\n",
    "for id in tqdm(range(len(distances))):\n",
    "    idx = sorted_ids[id][1:(n_next_hubs + 1)]\n",
    "    inward_flow_results[id] += compute_flows(distances[id, idx]).sum()\n",
    "\n",
    "# normalization and transformation\n",
    "rarity_score_flow = 1-inward_flow_results\n",
    "min_score = min(rarity_score_flow)\n",
    "max_score = max(rarity_score_flow)\n",
    "rarity_score_flow = (rarity_score_flow - min_score) / (max_score - min_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To be added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, there is a potential issue where the resulting rarity values are distributed too widely in the higher range, leading to a distortion in the distribution of values. As a result, data points that should be considered \"rare\" may appear in the lower range."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
